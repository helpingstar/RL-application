{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import gym_snakegame\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input.shape : (15, 15)\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 observation_space: spaces.MultiBinary,\n",
    "                 action_space: spaces.Discrete):\n",
    "        super().__init__()\n",
    "        assert len(\n",
    "            observation_space.shape) == 3, 'observation space must have the form channels x width x height'\n",
    "        assert type(\n",
    "            action_space) == spaces.Discrete, 'action_space must be of type Discrete'\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_features=64*7*7 , out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512, out_features=action_space.n)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0],-1)\n",
    "        return self.fc(conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Simple storage for transitions from an environment.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size):\n",
    "        \"\"\"\n",
    "        Initialise a buffer of a given size for storing transitions\n",
    "        :param size: the maximum number of transitions that can be stored\n",
    "        \"\"\"\n",
    "        self._storage = []\n",
    "        self._maxsize = size\n",
    "        self._next_idx = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._storage)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Add a transition to the buffer. Old transitions will be overwritten if the buffer is full.\n",
    "        :param state: the agent's initial state\n",
    "        :param action: the action taken by the agent\n",
    "        :param reward: the reward the agent received\n",
    "        :param next_state: the subsequent state\n",
    "        :param done: whether the episode terminated\n",
    "        \"\"\"\n",
    "        data = (state, action, reward, next_state, done)\n",
    "\n",
    "        if self._next_idx >= len(self._storage):\n",
    "            self._storage.append(data)\n",
    "        else:\n",
    "            self._storage[self._next_idx] = data\n",
    "        self._next_idx = (self._next_idx + 1) % self._maxsize\n",
    "\n",
    "    def _encode_sample(self, indices):\n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "        for i in indices:\n",
    "            data = self._storage[i]\n",
    "            state, action, reward, next_state, done = data\n",
    "            states.append(np.array(state, copy=False))\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(np.array(next_state, copy=False))\n",
    "            dones.append(done)\n",
    "        return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Randomly sample a batch of transitions from the buffer.\n",
    "        :param batch_size: the number of transitions to sample\n",
    "        :return: a mini-batch of sampled transitions\n",
    "        \"\"\"\n",
    "        indices = np.random.randint(0, len(self._storage) - 1, size=batch_size)\n",
    "        return self._encode_sample(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self,\n",
    "                 observation_space: spaces.MultiBinary,\n",
    "                 action_space: spaces.Discrete,\n",
    "                 replay_buffer: ReplayBuffer,\n",
    "                 lr,\n",
    "                 batch_size,\n",
    "                 gamma,\n",
    "                 device=torch.device(\"cpu\" )):\n",
    "        \"\"\"\n",
    "        Initialise the DQN algorithm using the Adam optimiser\n",
    "        :param action_space: the action space of the environment\n",
    "        :param observation_space: the state space of the environment\n",
    "        :param replay_buffer: storage for experience replay\n",
    "        :param lr: the learning rate for Adam\n",
    "        :param batch_size: the batch size\n",
    "        :param gamma: the discount factor\n",
    "        \"\"\"\n",
    "\n",
    "        self.memory = replay_buffer\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.policy_network = DQN(observation_space, action_space).to(device)\n",
    "        self.target_network = DQN(observation_space, action_space).to(device)\n",
    "        self.update_target_network()\n",
    "        self.target_network.eval()\n",
    "\n",
    "        self.optimiser = torch.optim.RMSprop(self.policy_network.parameters()\n",
    "            , lr=lr)        \n",
    "        ## self.optimiser = torch.optim.Adam(self.policy_network.parameters(), lr=lr)\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "    def optimise_td_loss(self):\n",
    "        \"\"\"\n",
    "        Optimise the TD-error over a single minibatch of transitions\n",
    "        :return: the loss\n",
    "        \"\"\"\n",
    "        device = self.device\n",
    "\n",
    "        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
    "        states = torch.from_numpy(states).float().to(device)\n",
    "        actions = torch.from_numpy(actions).long().to(device)\n",
    "        rewards = torch.from_numpy(rewards).float().to(device)\n",
    "        next_states = torch.from_numpy(next_states).float().to(device)\n",
    "        dones = torch.from_numpy(dones).float().to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(next_states)\n",
    "            max_next_q_values, _ = next_q_values.max(1)\n",
    "            target_q_values = rewards + (1 - dones) * self.gamma * max_next_q_values\n",
    "\n",
    "        input_q_values = self.policy_network(states)\n",
    "        input_q_values = input_q_values.gather(1, actions.unsqueeze(1)).squeeze()\n",
    "\n",
    "        loss = F.smooth_l1_loss(input_q_values, target_q_values)\n",
    "\n",
    "        self.optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimiser.step()\n",
    "        del states\n",
    "        del next_states\n",
    "        return loss.item()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        \"\"\"\n",
    "        Update the target Q-network by copying the weights from the current Q-network\n",
    "        \"\"\"\n",
    "        self.target_network.load_state_dict(self.policy_network.state_dict())\n",
    "\n",
    "    def act(self, state: np.ndarray):\n",
    "        \"\"\"\n",
    "        Select an action greedily from the Q-network given the state\n",
    "        :param state: the current state\n",
    "        :return: the action to take\n",
    "        \"\"\"\n",
    "        device = self.device\n",
    "        state = np.array(state) / 5.0\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.policy_network(state)\n",
    "            _, action = q_values.max(1)\n",
    "            return action.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_epsilon_decrease(now_step, total_step, max_eps=1, min_eps=0.1, ratio=0.1):\n",
    "    eps_timesteps = ratio * float(total_step)\n",
    "    fraction = min(1.0, float(now_step) / eps_timesteps)\n",
    "    eps = max_eps + fraction * (min_eps - max_eps)\n",
    "    return eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyTorchDimension(gym.ObservationWrapper):\n",
    "    \"\"\"Image shape to num_channels x height x width\"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super(PyTorchDimension, self).__init__(env)\n",
    "        shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=5, shape=(1, shape[0], shape[1]), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        shape = observation.shape\n",
    "        return observation.reshape(1, shape[0], shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardModifier(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        gym.RewardWrapper.__init__(self, env)\n",
    "\n",
    "    def reward(self, reward):\n",
    "        if reward == 0:\n",
    "            return -1\n",
    "        return reward * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\WooSeong\\anaconda3\\envs\\snakegame\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:35: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (15, 15)\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\WooSeong\\anaconda3\\envs\\snakegame\\lib\\site-packages\\gymnasium\\wrappers\\record_video.py:79: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\Users\\WooSeong\\RL-application\\video_folder folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00 -33.16\n",
      "0.00 -28.3\n",
      "0.00 -42.14\n",
      "0.00 -33.44\n",
      "0.00 -31.82\n",
      "0.00 -32.2\n",
      "0.01 -33.28\n",
      "0.01 -37.84\n",
      "0.01 -30.38\n",
      "0.01 -29.98\n",
      "0.01 -32.18\n",
      "0.01 -33.82\n",
      "0.01 -31.32\n",
      "0.01 -29.3\n",
      "0.01 -31.64\n",
      "0.01 -34.36\n",
      "0.01 -28.92\n",
      "0.01 -38.14\n",
      "0.01 -30.22\n",
      "0.01 -30.82\n",
      "0.02 -38.0\n",
      "0.02 -31.66\n",
      "0.02 -36.68\n",
      "0.02 -33.68\n",
      "0.02 -35.84\n",
      "0.02 -28.46\n",
      "0.02 -31.66\n",
      "0.02 -32.92\n",
      "0.02 -31.86\n",
      "0.02 -28.16\n",
      "0.02 -27.6\n",
      "0.02 -31.08\n",
      "0.02 -32.14\n",
      "0.03 -30.68\n",
      "0.03 -27.32\n",
      "0.03 -32.42\n",
      "0.03 -38.34\n",
      "0.03 -33.06\n",
      "0.03 -30.12\n",
      "0.03 -27.4\n",
      "0.03 -33.98\n",
      "0.03 -31.58\n",
      "0.03 -27.94\n",
      "0.03 -33.6\n",
      "0.03 -34.1\n",
      "0.03 -27.26\n",
      "0.03 -28.6\n",
      "0.04 -35.74\n",
      "0.04 -30.14\n",
      "0.04 -30.7\n",
      "0.04 -33.98\n",
      "0.04 -34.18\n",
      "0.04 -30.26\n",
      "0.04 -29.16\n",
      "0.04 -42.46\n",
      "0.04 -29.7\n",
      "0.04 -31.44\n",
      "0.04 -30.96\n",
      "0.04 -28.38\n",
      "0.04 -29.84\n",
      "0.04 -34.76\n",
      "0.05 -35.7\n",
      "0.05 -27.7\n",
      "0.05 -35.48\n",
      "0.05 -34.64\n",
      "0.05 -31.86\n",
      "0.05 -32.3\n",
      "0.05 -36.58\n",
      "0.05 -34.6\n",
      "0.05 -30.22\n",
      "0.05 -39.92\n",
      "0.05 -31.12\n",
      "0.05 -30.96\n",
      "0.05 -31.98\n",
      "0.06 -31.46\n",
      "0.06 -31.1\n",
      "0.06 -28.3\n",
      "0.06 -28.94\n",
      "0.06 -31.98\n",
      "0.06 -35.22\n",
      "0.06 -32.12\n",
      "0.06 -27.3\n",
      "0.06 -36.66\n",
      "0.06 -33.62\n",
      "0.06 -34.04\n",
      "0.06 -32.34\n",
      "0.06 -34.24\n",
      "0.07 -34.32\n",
      "0.07 -30.1\n",
      "0.07 -30.4\n",
      "0.07 -32.9\n",
      "0.07 -28.42\n",
      "0.07 -30.32\n",
      "0.07 -35.1\n",
      "0.07 -31.22\n",
      "0.07 -33.92\n",
      "0.07 -29.98\n",
      "0.07 -28.82\n",
      "0.07 -32.28\n",
      "0.07 -32.42\n",
      "0.07 -31.12\n",
      "0.08 -30.82\n",
      "0.08 -30.14\n",
      "0.08 -31.98\n",
      "0.08 -32.82\n",
      "0.08 -29.62\n",
      "0.08 -30.02\n",
      "0.08 -34.06\n",
      "0.08 -30.32\n",
      "0.08 -32.58\n",
      "0.08 -31.22\n",
      "0.08 -31.48\n",
      "0.08 -29.56\n",
      "0.08 -28.66\n",
      "0.08 -30.04\n",
      "0.09 -35.14\n",
      "0.09 -31.9\n",
      "0.09 -28.88\n",
      "0.09 -35.18\n",
      "0.09 -30.42\n",
      "0.09 -31.4\n",
      "0.09 -32.06\n",
      "0.09 -34.48\n",
      "0.09 -31.48\n",
      "0.09 -38.68\n",
      "0.09 -36.96\n",
      "0.09 -31.08\n",
      "0.09 -31.94\n",
      "0.10 -35.5\n",
      "0.10 -25.16\n",
      "0.10 -31.1\n",
      "0.10 -27.1\n",
      "0.10 -30.92\n",
      "0.10 -29.6\n",
      "0.10 -30.42\n",
      "0.10 -30.26\n",
      "0.10 -32.06\n",
      "0.10 -28.76\n",
      "0.10 -29.76\n",
      "0.10 -34.06\n",
      "0.10 -29.82\n",
      "0.10 -30.54\n",
      "0.10 -30.74\n",
      "0.11 -36.04\n",
      "0.11 -30.5\n",
      "0.11 -30.9\n",
      "0.11 -32.5\n",
      "0.11 -32.44\n",
      "0.11 -37.04\n",
      "0.11 -31.56\n",
      "0.11 -28.98\n",
      "0.11 -30.42\n",
      "0.11 -30.26\n",
      "0.11 -27.32\n",
      "0.11 -31.82\n",
      "0.11 -31.3\n",
      "0.11 -29.94\n",
      "0.12 -26.86\n",
      "0.12 -28.04\n",
      "0.12 -31.42\n",
      "0.12 -33.86\n",
      "0.12 -29.6\n",
      "0.12 -31.22\n",
      "0.12 -35.06\n",
      "0.12 -30.32\n",
      "0.12 -29.46\n",
      "0.12 -28.32\n",
      "0.12 -29.94\n",
      "0.12 -34.74\n",
      "0.12 -29.76\n",
      "0.12 -34.8\n",
      "0.13 -36.18\n",
      "0.13 -31.26\n",
      "0.13 -27.98\n",
      "0.13 -30.44\n",
      "0.13 -29.46\n",
      "0.13 -32.42\n",
      "0.13 -32.1\n",
      "0.13 -33.26\n",
      "0.13 -26.82\n",
      "0.13 -31.56\n",
      "0.13 -32.02\n",
      "0.13 -32.02\n",
      "0.13 -25.74\n",
      "0.13 -34.14\n",
      "0.14 -38.84\n",
      "0.14 -30.02\n",
      "0.14 -29.18\n",
      "0.14 -34.28\n",
      "0.14 -28.96\n",
      "0.14 -30.64\n",
      "0.14 -33.32\n",
      "0.14 -33.74\n",
      "0.14 -34.3\n",
      "0.14 -31.26\n",
      "0.14 -32.64\n",
      "0.14 -31.22\n",
      "0.14 -26.5\n",
      "0.14 -33.5\n",
      "0.15 -27.62\n",
      "0.15 -36.14\n",
      "0.15 -29.84\n",
      "0.15 -35.46\n",
      "0.15 -32.46\n",
      "0.15 -34.98\n",
      "0.15 -28.26\n",
      "0.15 -35.78\n",
      "0.15 -35.84\n",
      "0.15 -31.48\n",
      "0.15 -32.42\n",
      "0.15 -29.56\n",
      "0.15 -28.16\n",
      "0.16 -29.2\n",
      "0.16 -32.58\n",
      "0.16 -30.64\n",
      "0.16 -31.0\n",
      "0.16 -28.78\n",
      "0.16 -35.62\n",
      "0.16 -29.12\n",
      "0.16 -29.32\n",
      "0.16 -27.58\n",
      "0.16 -26.26\n",
      "0.16 -27.96\n",
      "0.16 -33.56\n",
      "0.16 -29.8\n",
      "0.16 -32.22\n",
      "0.16 -29.82\n",
      "0.17 -33.88\n",
      "0.17 -38.64\n",
      "0.17 -30.98\n",
      "0.17 -29.52\n",
      "0.17 -31.18\n",
      "0.17 -26.22\n",
      "0.17 -27.42\n",
      "0.17 -29.16\n",
      "0.17 -30.6\n",
      "0.17 -34.96\n",
      "0.17 -33.5\n",
      "0.17 -31.04\n",
      "0.17 -30.6\n",
      "0.17 -29.64\n",
      "0.18 -32.18\n",
      "0.18 -33.84\n",
      "0.18 -32.8\n",
      "0.18 -31.32\n",
      "0.18 -33.38\n",
      "0.18 -28.3\n",
      "0.18 -33.4\n",
      "0.18 -28.78\n",
      "0.18 -31.54\n",
      "0.18 -28.54\n",
      "0.18 -33.42\n",
      "0.18 -29.52\n",
      "0.18 -27.78\n",
      "0.18 -27.9\n",
      "0.19 -35.44\n",
      "0.19 -28.82\n",
      "0.19 -32.56\n",
      "0.19 -27.6\n",
      "0.19 -31.26\n",
      "0.19 -29.18\n",
      "0.19 -30.12\n",
      "0.19 -26.32\n",
      "0.19 -30.1\n",
      "0.19 -34.32\n",
      "0.19 -31.72\n",
      "0.19 -28.9\n",
      "0.19 -29.0\n",
      "0.19 -37.5\n",
      "0.20 -32.92\n",
      "0.20 -33.42\n",
      "0.20 -33.52\n",
      "0.20 -29.38\n",
      "0.20 -30.04\n",
      "0.20 -29.34\n",
      "0.20 -31.32\n",
      "0.20 -29.26\n",
      "0.20 -31.52\n",
      "0.20 -28.54\n",
      "0.20 -32.16\n",
      "0.20 -30.58\n"
     ]
    }
   ],
   "source": [
    "SEED_NUM = 20\n",
    "TOTAL_STEP = 2000000\n",
    "\n",
    "MAX_EPSILON = 1.0\n",
    "MIN_EPSILON = 0.1\n",
    "env = gym.make('gym_snakegame/SnakeGame-v0', size=15, n_target=1, render_mode='rgb_array')\n",
    "env = gym.wrappers.RecordVideo(env, video_folder='./video_folder', episode_trigger=lambda x: x % 200 == 0, name_prefix='snakegame', disable_logger=True)\n",
    "env = PyTorchDimension(env)\n",
    "env = RewardModifier(env)\n",
    "replay_buffer = ReplayBuffer(30000)\n",
    "\n",
    "agent = DQNAgent(env.observation_space,\n",
    "                 env.action_space,\n",
    "                 replay_buffer,\n",
    "                 lr=0.001,\n",
    "                 batch_size=64,\n",
    "                 gamma=0.99,\n",
    "                 device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "state, _ = env.reset()\n",
    "score = []\n",
    "episode_rewards = [0.0]\n",
    "step_count = 0\n",
    "for t in range(TOTAL_STEP):\n",
    "    sample = random.random()\n",
    "    epsilon = linear_epsilon_decrease(t, TOTAL_STEP, max_eps=1, min_eps=0.1, ratio=0.3)\n",
    "    if sample > epsilon:\n",
    "        # Exploit\n",
    "        action = agent.act(state)\n",
    "    else:\n",
    "        # Explore\n",
    "        action = env.action_space.sample()\n",
    "    next_state, reward, terminated, truncated, info = env.step(action)\n",
    "    agent.memory.add(next_state, action, reward, next_state, float(terminated))\n",
    "    state = next_state\n",
    "    episode_rewards[-1] += reward\n",
    "\n",
    "    if terminated:\n",
    "        state, _ = env.reset()\n",
    "        episode_rewards.append(0.0)\n",
    "        if len(episode_rewards) % 50 == 0:\n",
    "            print(f'{t/TOTAL_STEP:.2f} {sum(episode_rewards[-50:])/50}')\n",
    "\n",
    "    if t > 10000:\n",
    "        agent.optimise_td_loss()\n",
    "\n",
    "    if t > 10000 and t % 1000 == 0:\n",
    "        agent.update_target_network()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snakegame",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9772b7db8ff8d7825f5f46ea659e39dc2dbd3badfd87ca0de0fc78233fcb656e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
