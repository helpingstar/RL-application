{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original : https://github.com/DeepReinforcementLearning/DeepReinforcementLearningInAction/blob/master/Chapter%204/Ch4_book.ipynb\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project_path: CartPole-v1, device : cuda, run_name : REINFORCE_v1_1674898628\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    'env_id': 'CartPole-v1',\n",
    "    'algorithm': 'REINFORCE',\n",
    "    'algorithm_version': 'v1',\n",
    "    'truncated' : 500,\n",
    "    'seed': 42,\n",
    "    'cuda': True,\n",
    "    'learning_rate' : 0.0003,\n",
    "    'total_timesteps' : 300000,\n",
    "    'wandb_entity' : None,\n",
    "    'gamma' : 0.99,\n",
    "    'capture_video' : False,\n",
    "}\n",
    "\n",
    "project_path = args['env_id'].split('/')[-1]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and args[\"cuda\"] else \"cpu\")\n",
    "run_name=f\"{args['algorithm']}_{args['algorithm_version']}_{int(time.time())}\"\n",
    "\n",
    "print(f'project_path: {project_path}, device : {device}, run_name : {run_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "module() takes at most 2 arguments (3 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mQNetwork\u001b[39;00m(torch\u001b[39m.\u001b[39mnn):\n\u001b[0;32m      2\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, env:gym\u001b[39m.\u001b[39mEnv):\n\u001b[0;32m      3\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[1;31mTypeError\u001b[0m: module() takes at most 2 arguments (3 given)"
     ]
    }
   ],
   "source": [
    "class QNetwork(torch.nn):\n",
    "    def __init__(self, env:gym.Env):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(env.observation_space.shape[0], 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, env.action_space.n),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(args[\"env_id\"], render_mode=None)\n",
    "if args['truncated']:\n",
    "    env = gym.wrappers.TimeLimit(env, args['truncated'])\n",
    "env = gym.wrappers.AutoResetWrapper(env)\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find helpingstar.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33miamhelpingstar\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\helpi\\RL-application\\wandb\\run-20230127_202607-2zetp0yk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/iamhelpingstar/CartPole-v1/runs/2zetp0yk\" target=\"_blank\">REINFORCE_v1_1674818473</a></strong> to <a href=\"https://wandb.ai/iamhelpingstar/CartPole-v1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    name=run_name,\n",
    "    project=project_path,\n",
    "    entity=args['wandb_entity'],\n",
    "    # sync_tensorboard=True,\n",
    "    config=args,\n",
    "    monitor_gym=True,\n",
    "    save_code=True\n",
    ")\n",
    "\n",
    "writer = SummaryWriter(f'runs/{project_path}/{run_name}')\n",
    "writer.add_text(\n",
    "    \"hyperparameters\",\n",
    "    \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in args.items()])),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, gamma=0.99):\n",
    "    lenr = len(rewards)\n",
    "    #A Compute exponentially decaying rewards\n",
    "    disc_return = torch.pow(gamma,torch.arange(lenr).float()) * rewards\n",
    "    #B Normalize the rewards to be within the [0,1] interval to improve numerical stability\n",
    "    disc_return /= disc_return.max()\n",
    "    return disc_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A The loss function expects an array of action probabilities for the actions that were taken and the discounted rewards.\n",
    "def loss_fn(preds, r):\n",
    "    #B It computes the log of the probabilities, multiplies by the discounted rewards, sums them all and flips the sign.\n",
    "    return -1 * torch.sum(r * torch.log(preds)) #B"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$-\\sum \\gamma r_{t+1} \\log \\pi (a_t \\mid \\theta, S_t)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_network = QNetwork()\n",
    "optimizer = optim.Adam(q_network.parameters(), lr=args['learning_rate'])\n",
    "obs, _ = env.reset()\n",
    "for global_step in tqdm(range(args['total_timesteps'])):\n",
    "    obs, _ = env.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snakegame",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9a00f39f3a9ca9c7677cd04d781c0b87317aadbe3b89cd26c4b51ffd90ae63f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
